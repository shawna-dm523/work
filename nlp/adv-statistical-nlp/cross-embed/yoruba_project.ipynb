{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697ed9e-7edb-46f5-83cd-d6d1a5111ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b85f2-c1ee-4939-9454-d516bf0e7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_split_train_csv(input_file, output_yoruba, output_english, rows=250):\n",
    "    \"\"\"\n",
    "    Processes the train.csv file to create separate Yoruba and English CSVs.\n",
    "    :param input_file: Path to the input train.csv file.\n",
    "    :param output_yoruba: Path to save the Yoruba sentences CSV.\n",
    "    :param output_english: Path to save the English sentences CSV.\n",
    "    :param rows: Number of rows to process (default is 250).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data = pd.read_csv(input_file)\n",
    "    \n",
    "    processed_data = train_data[['Yoruba', 'English']].head(rows)\n",
    "    \n",
    "    processed_data[['Yoruba']].to_csv(output_yoruba, index=False, header=False)\n",
    "    print(f\"Yoruba sentences saved to {output_yoruba}\")\n",
    "    \n",
    "    processed_data[['English']].to_csv(output_english, index=False, header=False)\n",
    "    print(f\"English sentences saved to {output_english}\")\n",
    "\n",
    "input_file = 'train.csv' \n",
    "output_yoruba = 'yoruba_sentences.csv'  \n",
    "output_english = 'english_sentences.csv' \n",
    "\n",
    "process_and_split_train_csv(input_file, output_yoruba, output_english, rows=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d2c17-6448-4726-be0b-60fad912b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_models():\n",
    "    print(\"Downloading and loading FastText models...\")\n",
    "    en_model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\")\n",
    "    en_model = fasttext.load_model(en_model_path)\n",
    "\n",
    "    yo_model_path = hf_hub_download(repo_id=\"facebook/fasttext-yo-vectors\", filename=\"model.bin\")\n",
    "    yo_model = fasttext.load_model(yo_model_path)\n",
    "\n",
    "    return en_model, yo_model\n",
    "\n",
    "def load_mbert_model():\n",
    "    print(\"Loading mBERT model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def get_mbert_embedding(model, tokenizer, text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy().flatten()\n",
    "\n",
    "def evaluate_fasttext_sentences(yo_model, en_model, yoruba_sentences, english_sentences):\n",
    "    results = []\n",
    "    for yoruba_sentence in yoruba_sentences:\n",
    "        try:\n",
    "            yo_vec = np.mean([yo_model.get_word_vector(word) for word in yoruba_sentence.split()], axis=0)\n",
    "            closest_sentence = sorted(\n",
    "                english_sentences,\n",
    "                key=lambda en_sentence: cosine(\n",
    "                    yo_vec,\n",
    "                    np.mean([en_model.get_word_vector(word) for word in en_sentence.split()], axis=0)\n",
    "                )\n",
    "            )[0]\n",
    "            similarity = 1 - cosine(\n",
    "                yo_vec,\n",
    "                np.mean([en_model.get_word_vector(word) for word in closest_sentence.split()], axis=0)\n",
    "            )\n",
    "            results.append((yoruba_sentence, closest_sentence, similarity))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence '{yoruba_sentence}': {e}\")\n",
    "            results.append((yoruba_sentence, \"Error\", 0))\n",
    "    return results\n",
    "\n",
    "def evaluate_mbert_sentences(yoruba_sentences, english_sentences, tokenizer, model):\n",
    "    results = []\n",
    "    for yoruba_sentence in yoruba_sentences:\n",
    "        try:\n",
    "            yo_vec = get_mbert_embedding(model, tokenizer, yoruba_sentence)\n",
    "            closest_sentence = sorted(\n",
    "                english_sentences,\n",
    "                key=lambda en_sentence: cosine(\n",
    "                    yo_vec,\n",
    "                    get_mbert_embedding(model, tokenizer, en_sentence)\n",
    "                )\n",
    "            )[0]\n",
    "            similarity = 1 - cosine(yo_vec, get_mbert_embedding(model, tokenizer, closest_sentence))\n",
    "            results.append((yoruba_sentence, closest_sentence, similarity))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sentence '{yoruba_sentence}': {e}\")\n",
    "            results.append((yoruba_sentence, \"Error\", 0))\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    yoruba_sentences = pd.read_csv('yoruba_sentences.csv', header=None).squeeze().tolist()\n",
    "    english_sentences = pd.read_csv('english_sentences.csv', header=None).squeeze().tolist()\n",
    "\n",
    "    en_model, yo_model = load_fasttext_models()\n",
    "    tokenizer, mbert_model = load_mbert_model()\n",
    "\n",
    "    print(\"Evaluating FastText with Yoruba and English sentences...\")\n",
    "    fasttext_sentence_results = evaluate_fasttext_sentences(yo_model, en_model, yoruba_sentences, english_sentences)\n",
    "\n",
    "    print(\"Evaluating mBERT with Yoruba and English sentences...\")\n",
    "    mbert_sentence_results = evaluate_mbert_sentences(yoruba_sentences, english_sentences, tokenizer, mbert_model)\n",
    "\n",
    "    sentence_comparison_df = pd.DataFrame({\n",
    "        \"Yoruba Sentence\": yoruba_sentences,\n",
    "        \"FastText Match\": [row[1] for row in fasttext_sentence_results],\n",
    "        \"FastText Similarity\": [row[2] for row in fasttext_sentence_results],\n",
    "        \"mBERT Match\": [row[1] for row in mbert_sentence_results],\n",
    "        \"mBERT Similarity\": [row[2] for row in mbert_sentence_results]\n",
    "    })\n",
    "    sentence_comparison_df.to_csv(\"yoruba_english_sentence_comparison.csv\", index=False)\n",
    "\n",
    "    print(\"Sentence-level comparison complete. Results saved to 'yoruba_english_sentence_comparison.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a5bcd-961c-472c-a05c-4410a1376296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentence_similarity_bar_graphs(comparison_df):\n",
    "    \"\"\"\n",
    "    Plots bar graphs comparing FastText and mBERT cosine similarity scores with generic labels.\n",
    "    \"\"\"\n",
    "    num_sentences = len(comparison_df)\n",
    "    labels = [f\"Sentence {i+1}\" for i in range(num_sentences)]\n",
    "    fasttext_scores = comparison_df[\"FastText Similarity\"]\n",
    "    mbert_scores = comparison_df[\"mBERT Similarity\"]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(x - width/2, fasttext_scores, width, label=\"FastText\")\n",
    "    plt.bar(x + width/2, mbert_scores, width, label=\"mBERT\")\n",
    "\n",
    "    plt.xlabel(\"Sentences\")\n",
    "    plt.ylabel(\"Cosine Similarity\")\n",
    "    plt.title(\"Comparison of FastText and mBERT Similarities for Sentences\")\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sentence_similarity_bar_graphs(sentence_comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b2b2a-1e42-4499-9d78-e3ddab89e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "comparison_df = pd.read_csv(\"yoruba_english_sentence_comparison.csv\")\n",
    "\n",
    "fasttext_scores = comparison_df[\"FastText Similarity\"]\n",
    "mbert_scores = comparison_df[\"mBERT Similarity\"]\n",
    "\n",
    "t_stat, p_value = ttest_ind(fasttext_scores, mbert_scores)\n",
    "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The difference between FastText and mBERT is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference between FastText and mBERT is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520c26b-7819-46f9-8338-c6415b6b337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_random_subset(yoruba_sentences, english_sentences, subset_size=10, num_runs=5):\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        sampled_indices = np.random.choice(len(yoruba_sentences), subset_size, replace=False)\n",
    "        sampled_yoruba = [yoruba_sentences[i] for i in sampled_indices]\n",
    "        sampled_english = [english_sentences[i] for i in sampled_indices]\n",
    "        \n",
    "        fasttext_results = evaluate_fasttext_sentences(yo_model, en_model, sampled_yoruba, sampled_english)\n",
    "        fasttext_avg_similarity = np.mean([row[2] for row in fasttext_results])\n",
    "        \n",
    "        mbert_results = evaluate_mbert_sentences(sampled_yoruba, sampled_english, tokenizer, mbert_model)\n",
    "        mbert_avg_similarity = np.mean([row[2] for row in mbert_results])\n",
    "        \n",
    "        results.append((fasttext_avg_similarity, mbert_avg_similarity))\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\"FastText Avg Similarity\", \"mBERT Avg Similarity\"])\n",
    "\n",
    "robustness_results = evaluate_on_random_subset(yoruba_sentences, english_sentences, subset_size=10, num_runs=5)\n",
    "print(robustness_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225566d-446e-42f9-a681-b4549529aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "yoruba_embeddings = [get_mbert_embedding(mbert_model, tokenizer, text) for text in yoruba_sentences]\n",
    "english_embeddings = [get_mbert_embedding(mbert_model, tokenizer, text) for text in english_sentences]\n",
    "\n",
    "all_embeddings = np.vstack(yoruba_embeddings + english_embeddings)\n",
    "labels = [\"Yoruba\"] * len(yoruba_embeddings) + [\"English\"] * len(english_embeddings)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(all_embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for label in set(labels):\n",
    "    idx = [i for i, l in enumerate(labels) if l == label]\n",
    "    plt.scatter(reduced_embeddings[idx, 0], reduced_embeddings[idx, 1], label=label)\n",
    "plt.legend()\n",
    "plt.title(\"PCA Visualization of mBERT Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a6dfa-c1d3-4ad1-886b-c1644bee040a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
